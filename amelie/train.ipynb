{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/u/nikcaryo/software/miniconda/envs/amelie/bin/python\n",
      "/cluster/u/nikcaryo/amelie-229/amelie\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloaders.hpo\n",
      "<function parent_package_name at 0x7f156f5f15f0>\n",
      "text_classification\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import importlib\n",
    "import constdb\n",
    "\n",
    "import text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_article(processed_dir, pmid):\n",
    "    path = processed_dir + '/' + str(pmid) + '.pkl'\n",
    "\n",
    "    if not os.path.isfile(path):\n",
    "        return None\n",
    "    else:\n",
    "        with open(path, 'rb') as file:\n",
    "            try:\n",
    "                loaded = pickle.load(file)\n",
    "                return loaded\n",
    "            except EOFError:\n",
    "                print(\"Cannot load pmid %s for some reason ...\" % pmid, flush=True)\n",
    "                return None\n",
    "\n",
    "class ConvertToTextFunction:\n",
    "    def __init__(self, processed_dir, replace_phenos_with_nothing):\n",
    "        self.processed_dir = processed_dir\n",
    "        self.replace_phenos_with_nothing = replace_phenos_with_nothing\n",
    "\n",
    "    def __call__(self, pmid):\n",
    "        processed_article = load_article(self.processed_dir, pmid)\n",
    "        if processed_article is None:\n",
    "            return None\n",
    "\n",
    "        article = text_classification.convert_to_text(processed_article, use_main_text=True,\n",
    "                                                      replace_phenos_with_nothing=self.replace_phenos_with_nothing)\n",
    "\n",
    "        return pmid, article\n",
    "\n",
    "\n",
    "def convert_all_to_text(processed_dir, pmids, replace_phenos_with_nothing):\n",
    "    with multiprocessing.Pool(100) as pool:\n",
    "        print('Pool created', flush=True)\n",
    "        for i, item in enumerate(pool.imap_unordered(\n",
    "                ConvertToTextFunction(processed_dir,\n",
    "                                      replace_phenos_with_nothing=replace_phenos_with_nothing),\n",
    "                pmids,\n",
    "                chunksize=100)):\n",
    "            if i % 10000 == 0:\n",
    "                print('Processed ', i, ' out of ', len(pmids), flush=True)\n",
    "\n",
    "            if item is not None:\n",
    "                yield item\n",
    "\n",
    "def shuffle_articles_labels(articles, labels):\n",
    "    zipped_articles_labels = [x for x in zip(articles, labels)]\n",
    "    random.shuffle(zipped_articles_labels)\n",
    "    return [x for x in zip(*zipped_articles_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_text_field_classifier(out_dir, process_dir, field_name, save, cross_val, l1,\n",
    "                                replace_phenos_with_nothing):\n",
    "    with open(out_dir + '/dataset_meta.json') as file:\n",
    "        dataset_info = json.load(file)\n",
    "\n",
    "    with open(out_dir + '/omim.json') as file:\n",
    "        omim = json.load(file)\n",
    "    positives = set(dataset_info['positive_pmids'])\n",
    "    print('Total positives: ', len(positives), flush=True)\n",
    "    good_positives = []\n",
    "\n",
    "    for pmid in positives:\n",
    "        omim_data = omim[str(pmid)]\n",
    "        if field_name not in omim_data:\n",
    "            continue\n",
    "        field = omim_data[field_name]\n",
    "        if len(field) == 0 or len(field) == 2:\n",
    "            # Skip bad ones\n",
    "            continue\n",
    "        good_positives.append(pmid)\n",
    "    \n",
    "    print('Good positives: ', len(good_positives), flush=True)\n",
    "    articles = []\n",
    "    labels = []\n",
    "\n",
    "    for pmid, article in convert_all_to_text(process_dir, good_positives,\n",
    "                                             replace_phenos_with_nothing=replace_phenos_with_nothing):\n",
    "        omim_data = omim[str(pmid)]\n",
    "        field = omim_data[field_name]\n",
    "        articles.append(article)\n",
    "        labels.append(field[0])\n",
    "\n",
    "    for pmid, article in convert_all_to_text(process_dir, dataset_info['gwas_pmids'],\n",
    "                                             replace_phenos_with_nothing=replace_phenos_with_nothing):\n",
    "        articles.append(article)\n",
    "        labels.append('gwas')\n",
    "\n",
    "    articles, labels = shuffle_articles_labels(articles, labels)\n",
    "    \n",
    "    # remove for actual training!\n",
    "    articles = articles[:100]\n",
    "    labels = labels[:100]\n",
    "\n",
    "    print('Have ', len(articles), flush=True)\n",
    "    counters = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counters[label] += 1\n",
    "\n",
    "    print('Counts per label:', counters, flush=True)\n",
    "    print('Done converting', flush=True)\n",
    "    classifier = text_classification.create_model(articles, labels, cross_val=cross_val, l1=l1)\n",
    "\n",
    "    if save:\n",
    "        print(\"SAVING TEXT FIELD RELEVANCE CLASSIFIER\", flush=True)\n",
    "        with open(out_dir + '/text_field_{}.pkl'.format(field_name), 'wb') as out_file:\n",
    "            pickle.dump(classifier, out_file)\n",
    "    else:\n",
    "        print(\"NOT SAVING TEXT FIELD RELEVANCE CLASSIFIER\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positives:  60160\n",
      "Good positives:  11177\n",
      "Pool created\n",
      "Processed  0  out of  11177\n",
      "Processed  10000  out of  11177\n",
      "Pool created\n",
      "Processed  0  out of  3264\n",
      "Have  100\n",
      "Counts per label: defaultdict(<class 'int'>, {'recessive': 43, 'gwas': 24, 'dominant': 33})\n",
      "Done converting\n",
      "Pipeline created\n",
      "Five-fold cross validation\n",
      "Scoring: micro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/cluster/u/nikcaryo/software/miniconda/envs/amelie/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/cluster/u/nikcaryo/software/miniconda/envs/amelie/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: \n",
      "{'fit_time': array([0.50311828, 0.39474034, 0.39328837, 0.37328362, 0.3978343 ]), 'score_time': array([0.13898945, 0.16097331, 0.16253185, 0.19937515, 0.15924454]), 'test_precision': array([0.52380952, 0.66666667, 0.66666667, 0.68421053, 0.5       ]), 'train_precision': array([1., 1., 1., 1., 1.]), 'test_recall': array([0.52380952, 0.66666667, 0.66666667, 0.68421053, 0.5       ]), 'train_recall': array([1., 1., 1., 1., 1.])}\n",
      "Fitting classifier\n",
      "SAVING TEXT FIELD RELEVANCE CLASSIFIER\n"
     ]
    }
   ],
   "source": [
    "out_dir = \"amelie_out_dir\"\n",
    "process_dir = \"amelie_process_dir\"\n",
    "\n",
    "save = True\n",
    "cross_val = True\n",
    "l1=False\n",
    "replace_phenos_with_nothing=False\n",
    "\n",
    "# right now it's set to only use 100 papers\n",
    "# training on all of the papers is p slow\n",
    "train_text_field_classifier(out_dir, process_dir, \"inheritance_modes\", save, cross_val, l1,\n",
    "                                replace_phenos_with_nothing)\n",
    "# train_text_field_classifier(out_dir, process_dir, \"variant_types\", save, cross_val, l1,\n",
    "#                                 replace_phenos_with_nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
